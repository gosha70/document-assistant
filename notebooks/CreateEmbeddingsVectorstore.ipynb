{"cells":[{"cell_type":"markdown","metadata":{"id":"ZWaCbmCLclrq"},"source":["# Create Embeddings Vectorstore\n","\n","This is a Collab script creates a [Chroma](https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.chroma.Chroma.html)  vector store from the specified **zip** file, which includes `JSON` files with splits of unstructured documents.\n","\n","Use the following [script](https://github.com/gosha70/document-assistant/blob/main/embeddings/document_loader.py) to scan and proocess local documents. After the cloning the [document-assistant](https://github.com/gosha70/document-assistant), navigate into the cloned repo:\n","\n","```\n","> python3 -m embeddings.document_loader --dir_path DOCUMENT_FOLDER --file_type FILE_TYPE --file_type FILE_NAME_PATTERN --persist_directory OUTPUT_FOLDER\n","```\n","\n","Where:\n","\n","\n","* `--dir_path`- (optional) the root directory where to look for documents. The default is `\".\"`\n","* `--file_type`- the list of file extensions:\n"," * `md`\n"," * `java`\n"," * `xml`\n"," * `html`\n"," * `pdf`\n","\n","* `--file_patterns` - (optional) file name patterns for each file type; for example: `--file_patterns \"java:**/*Function.java\" \"java:**/*Api*\"`\n","\n","\n","* `--persist_directory` - (optional) the path to the directory where unstructured document splits are saved.\n","<br/><br/>\n","<br/><br/>\n",">\n","> &copy; **EGOGE** - All Rights Reserved.\n",">\n","> _This software may be used and distributed according to the terms of the CC-BY-SA-4.0 license._\n","<br/><br/>"]},{"cell_type":"markdown","metadata":{"id":"LgnHjQWFbk-x"},"source":["### Installation\n","---\n","Install required python libraries:\n","\n","\n","1.   [LangChain](https://python.langchain.com/docs/modules/chains/foundational/llm_chain)\n","\n","2. [ChromaDB](https://docs.trychroma.com/)\n","\n","3. [Huggingface](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.huggingface.HuggingFaceInstructEmbeddings.html)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSr7otKWWwvF"},"outputs":[],"source":["# Install lingchain and embedding libraries\n","!pip install langchain transformers tqdm sentence_transformers InstructorEmbedding huggingface huggingface_hub chromadb"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"UHcy_D2MbMLC"},"source":["### Map Google Drive\n","---\n","Map to Google Drive where a zip file with Documents splits is located"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1c24yFQbZcx"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"3oI1Xu3ddmQc"},"source":["## Unzip Documents\n","\n","Set the following variables:\n","\n","\n","*   `zip_path`: the full path to the zip file in the mounted Google Drive\n","*   `extract_dir`: the full path to the directory where extracted files are saved\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1703377479587,"user":{"displayName":"George Ivan","userId":"15934351572926549385"},"user_tz":300},"id":"H5Xm3bWwd1Y2"},"outputs":[],"source":["import zipfile\n","import os\n","# Specify a full path to the zip file:\n","zip_path = '/content/drive/MyDrive/DOCS_ZIP.zip'\n","\n","# Specify a directory to extract the zip file to:\n","extract_dir = '/content/drive/MyDrive/UNZIP_FOLDER'\n","\n","# Specify a directory to cache the embedding data.\n","# Comment out this lien. if you want to disable the Transformer Cache.\n","os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/TRANSFORMER_CACHE'\n","\n","def unzip_documents(zip_file, unzip_folder):\n","  # Create the directory if it doesn't exist\n","  os.makedirs(extract_dir, exist_ok=True)\n","\n","  # Open the zip file\n","  with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","      # Extract all the contents into the directory\n","      zip_ref.extractall(extract_dir)\n","\n","# Unzip documents.\n","# If you run the script a few times, - comment out this line after the first run.\n","unzip_documents(zip_file=zip_path, unzip_folder=extract_dir)"]},{"cell_type":"markdown","metadata":{"id":"p6rn0RNUexkf"},"source":["## JSON to Document\n","\n","The conversion utility to convert the textual presentation of `Document` stored in a loaded **JSON** into in the in-memory `Document`.\n","\n","Here is the example of *jsonified* `Document`:\n","```\n","{\n","    \"lc\": 1,\n","    \"type\": \"constructor\",\n","    \"id\": [\n","        \"langchain_core\",\n","        \"documents\",\n","        \"base\",\n","        \"Document\"\n","    ],\n","    \"kwargs\": {\n","        \"page_content\": \"public class Main {}\\n\\n}\",\n","        \"metadata\": {\n","            \"source\": \"Main.java\"\n","        }\n","    }\n","}\n","```"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":164,"status":"ok","timestamp":1703377479745,"user":{"displayName":"George Ivan","userId":"15934351572926549385"},"user_tz":300},"id":"3w0KPdc_bURY"},"outputs":[],"source":["import json\n","from langchain_core.documents import Document\n","\n","def load_document(file) -> Document:\n","    \"\"\"\n","    Create a Document from the specified JSON file.\n","\n","    Parameters:\n","    - file (File): the JSON file\n","\n","    Returns (Document)\n","    \"\"\"\n","    try:\n","        # Read and process the content as JSON\n","        json_content = file.read()\n","\n","        # Parse the JSON content\n","        data = json.loads(json_content)\n","\n","        # Access the \"page_content\" field\n","        page_content = data['kwargs']['page_content']\n","\n","        # Access the \"metadata\" field\n","        metadata = data['kwargs']['metadata']\n","\n","        # Transform the data into a langchain_core.documents.Document\n","        # Assuming the JSON structure fits the Document's requirements\n","        return Document(page_content=page_content, metadata=metadata)\n","    except Exception as error:\n","        print(f\"File {file} is not a valid JSON: {str(error)}\")\n","\n","    return None"]},{"cell_type":"markdown","metadata":{"id":"ZcycLUlvfpbM"},"source":["## Create Embedding LLM\n","\n","1. Update `EMBEDDING_KWARGS` the device type based on the selected runtime environment: `cuda` or `cpu`\n","\n","2. Update `DEFAULT_MODEL_NAME` with the model name of embedding LLM. The choosen model must correspond to a model will be use later in a runtime Application. Both models must have the same dimensionality: the number of dimensions in which the vector representation of a word is defined; the total number of features that are encoded in the vector representation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WhUWHe_EfqFH"},"outputs":[],"source":["from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n","from tqdm.auto import tqdm\n","\n","# You have to choose GPU in order to use CUDA\n","EMBEDDING_KWARGS = {'device': 'cpu'} # 'cuda' or 'cpu'\n","ENCODE_KWARG = {'normalize_embeddings': True}\n","\n","def create_embedding(model_name):\n","    return HuggingFaceInstructEmbeddings(\n","        model_name=model_name,\n","        model_kwargs=EMBEDDING_KWARGS,\n","        encode_kwargs=ENCODE_KWARG\n","    )\n","\n","DEFAULT_MODEL_NAME = \"hkunlp/instructor-large\"\n","\n","# Create Embedding LLM\n","embedding = create_embedding(model_name=DEFAULT_MODEL_NAME)"]},{"cell_type":"markdown","metadata":{"id":"BKk2K4rxfq_u"},"source":["## Create/Update Chroma Vectorstore\n","\n","\n","Customize the following variables:\n","*   `persist_directory`: the folder where the **Chroma** vectorstore is located\n","*   `collection_name`: the name of the **Chroma** vectorstore\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":542,"status":"ok","timestamp":1703377491592,"user":{"displayName":"George Ivan","userId":"15934351572926549385"},"user_tz":300},"id":"ite96x6XfPEe"},"outputs":[],"source":["import chromadb\n","from datetime import datetime\n","from chromadb.config import Settings\n","from langchain_community.vectorstores import Chroma\n","\n","def create_vectorestore(embedding, documents) -> Chroma:\n","  # Specify the location in Google Drive where the new Chroma vectorstore is saved.\n","  persist_directory = '/content/drive/MyDrive/CHROMA_DB/'\n","  # Specify the name of the new Chroma vectorstore\n","  collection_name=\"EGOGE_DOCUMENTS_DB\"\n","  print(f\"Creating the embedding vectorstore with {embedding}\\n with {len(documents)} document splits ...\")\n","  docs_db = Chroma.from_documents(\n","      documents=documents,\n","      collection_name=collection_name\n","      embedding=embedding,\n","      persist_directory=persist_directory\n","  )\n","\n","  # Save the Chroma database after processing each chunk\n","  print(f\"Saving the Chroma vectorstore with {len(documents)} documents  ...\")\n","  docs_db.persist()\n","  \n","  meta_inf_path = os.path.join(persist_directory, 'META-INF')\n","  manifest_file_path = os.path.join(meta_inf_path, 'MANIFEST.MF')\n","  \n","  # Get current UTC time and format it\n","  current_utc_datetime = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z' \n","\n","  # Manifest content\n","  manifest_content = f\"\"\"\n","  Manifest-Version: 1.0\n","  Created-On: {current_utc_datetime}\n","  Created-By: EGOGE (https://github.com/gosha70/document-assistant)\n","  Collection Name: {collection_name}\n","  Embedding Class: {\"langchain_community.embeddings.HuggingFaceInstructEmbeddings\"}\n","  Embedding Model Name: {DEFAULT_MODEL_NAME}\n","  \"\"\"\n","\n","  if not os.path.exists(meta_inf_path):\n","      os.makedirs(meta_inf_path)\n","      print(f\"Created directory: {meta_inf_path}\")\n","\n","  with open(manifest_file_path, 'w') as file:\n","      file.write(manifest_content)\n","      print(f\"Created MANIFEST.MF at: {manifest_file_path}\")\n","\n","  return docs_db\n","\n","def process_documents_in_chunks(embedding, file_paths, chunk_size) -> Chroma:\n","  docs_db = None\n","  total_count = len(file_paths)\n","  # Process in chunks\n","  for i in range(0, len(file_paths), chunk_size):\n","      chunk = file_paths[i:i + chunk_size]\n","      documents = []\n","\n","      # Process each file in the chunk\n","      for file_path in chunk:\n","          with open(file_path, 'r') as file:\n","              doc = load_document(file)\n","              if doc is not None:\n","                  documents.append(doc)\n","\n","      batch_id = f\"{i // chunk_size + 1}/{total_count // chunk_size + 1}\"\n","      print(f\"Processing the batch {batch_id}: {len(documents)} documents\")\n","      if documents:\n","          if docs_db is None:\n","            docs_db = create_vectorestore(embedding=embedding, documents=documents)\n","          else:\n","            print(f\"Updating the embedding vectorstore with {len(documents)} document splits ...\")\n","            docs_db.add_documents(documents=documents)\n","            # Save the Chroma database after processing each chunk\n","            print(f\"Finished the batch {batch_id}.\")\n","\n","  docs_db.persist()\n","  return docs_db"]},{"cell_type":"markdown","metadata":{"id":"Y92JKE-cfOh2"},"source":["## Process All Documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k4o5bRM0dt43"},"outputs":[],"source":["CHUNK_SIZE = 1000  # Number of files to process at a time\n","\n","# Collect all file paths\n","file_paths = []\n","for dirpath, dirnames, filenames in os.walk(extract_dir):\n","    for file_name in filenames:\n","        full_path = os.path.join(dirpath, file_name)\n","        file_paths.append(full_path)\n","\n","filesCount = len(file_paths)\n","print(f\"Found {filesCount} files will be processed in {CHUNK_SIZE} batches ...\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWZqkKmwzBPL"},"outputs":[],"source":["if filesCount > 0:\n","  chroma_db = process_documents_in_chunks(embedding, file_paths, CHUNK_SIZE)\n","  dic = chroma_db.get()[\"ids\"]\n","  print(f\"Finished the creation of the Chroma vectorstore with {len(dic)} documents:\\n {chroma_db}\")\n","else:\n","  print(\"ERROR: Cannot create the Chroma vectorstore without documents.\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOT9NbiooYFkG6yCO0BT2u9","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
